{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-04aa5bc589e8>:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working\n",
      "  from collections import Iterable\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import gzip\n",
    "from collections import Iterable\n",
    "import configparser\n",
    "\n",
    "_ENCODING_UTF8 = 'utf-8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonline(filename, encoding=_ENCODING_UTF8, default=None, is_gzip=False):\n",
    "    \"\"\"\n",
    "    read jsonl file\n",
    "    :param filename: source file path\n",
    "    :param encoding: file encoding\n",
    "    :param default: returned value when filename is not existed.\n",
    "                    If it's None, exception will be raised as usual.\n",
    "    :param is_gzip: whether input file is gzip format\n",
    "    :return: object list, an object corresponding a line\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filename) and default is not None:\n",
    "        return default\n",
    "    if not is_gzip:\n",
    "        file = open(filename, encoding=encoding)\n",
    "    else:\n",
    "        file = gzip.open(filename, 'rt', encoding=encoding)\n",
    "    items = []\n",
    "    for line in file:\n",
    "        items.append(json.loads(line))\n",
    "    file.close()\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json(filename, data, serialize_method=None):\n",
    "    \"\"\"\n",
    "    dump json data to file, support non-UTF8 string (will not occur UTF8 hexadecimal code).\n",
    "    :param filename: destination file path\n",
    "    :param data: data to be saved\n",
    "    :param serialize_method: python method to do serialize method\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    with open(filename, 'a', encoding=_ENCODING_UTF8) as f:\n",
    "        if not serialize_method:\n",
    "            json.dump(data, f, ensure_ascii=False)\n",
    "        else:\n",
    "            json.dump(data, f, ensure_ascii=False, default=serialize_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_line(filename, line, encoding=_ENCODING_UTF8):\n",
    "    \"\"\"\n",
    "    append single line to file\n",
    "    :param filename: destination file path\n",
    "    :param line: line string\n",
    "    :param encoding: text encoding to save data\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if not isinstance(line, str):\n",
    "        raise TypeError('line is not in str type')\n",
    "    with open(filename, 'a', encoding=encoding) as f:\n",
    "        f.write(line + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据的标准格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[abstract]\n",
      "A feedback vertex set of a graph G is a set S  of its vertices such that the subgraph induced by V(G)?S V ( G ) ? S is a forest. The cardinality of a minimum feedback vertex set of G  is denoted by ?(G) ? ( G ) . A graph G is 2-degenerate  if each subgraph G? G ? of G has a vertex v  such that dG?(v)?2 d G ? ( v ) ? 2 . In this paper, we prove that ?(G)?2n/5 ? ( G ) ? 2 n / 5 for any 2-degenerate n-vertex graph G and moreover, we show that this bound is tight. As a consequence, we derive a polynomial time algorithm, which for a given 2-degenerate n -vertex graph returns its feedback vertex set of cardinality at most 2n/5 2 n / 5 .\n",
      "[keyword]\n",
      "feedback vertex set;decycling set;2-degenerate graphs\n",
      "[title]\n",
      "A feedback vertex set of 2-degenerate graphs\n"
     ]
    }
   ],
   "source": [
    "dict_list_kp20k = read_jsonline('/mnt/KeyphraseExpansion/data/raw/kp20k/kp20k_testing.json')\n",
    "for k,v in dict_list_kp20k[0].items():\n",
    "    print('['+k+']')\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据现在的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_list = read_jsonline('/mnt/KeyphraseExpansion/data/raw/semeval/semeval_testno.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name]\n",
      "test_C-14\n",
      "[title]\n",
      "Sensor Deployment Strategy for Target Detection\n",
      "[abstract]\n",
      "In order to monitor a region for traffic traversal, sensors can be deployed to perform collaborative target detection. Such a sensor network achieves a certain level of detection performance with an a\n",
      "[fulltext]\n",
      "1. INTRODUCTION\n",
      "Recent advances in computing hardware and software are\n",
      "responsible for the emergence of sensor networks capable of\n",
      "observing the environment, processing the data and making\n",
      "decisions b\n",
      "[keywords]\n",
      "exposure;sensor number;path exposure;deployment;target detection;target decay;sequential deployment;value fusion;sensor network;random sensor placement;number of sensor;minimum exposure;sensor field;c\n"
     ]
    }
   ],
   "source": [
    "for k,v in dict_list[1].items():\n",
    "    print('['+k+']')\n",
    "    print(v[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 改变数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = []\n",
    "for dic in dict_list:\n",
    "    new_dict = {}\n",
    "    new_dict[\"abstract\"] = str(dic['abstract']).replace('`','').replace(\"'\",'')\n",
    "    new_dict[\"keyword\"] = dic['keywords']\n",
    "    new_dict[\"title\"] = dic['title']\n",
    "    new_list.append(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 写成文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dic in new_list:\n",
    "    filename = '/mnt/KeyphraseExpansion/data/raw/semeval/semeval_test.json'\n",
    "    append_line(filename, json.dumps(dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============title_and_abstract_tokens===========\n",
      "['A', 'new', 'method', 'of', 'systemological', 'analysis', 'coordinated', 'with', 'the', 'procedure', 'ofobject-oriented', 'design.', 'II', 'For', 'pt.I.', 'see', 'Vestn.', 'KhGPU,', 'no.81,', 'p.15-18', '(2000).', 'The', 'paper', 'presents', 'the', 'results', 'of', 'development', 'of', 'an', 'object-oriented', 'systemological', 'method', 'used', 'to', 'design', 'complex', 'systems.', 'A', 'formal', 'system', 'representation,', 'as', 'well', 'as', 'an', 'axiomatics', 'of', 'the', 'calculus', 'of', 'systems', 'as', 'functional', 'flow-type', 'objects', 'based', 'on', 'a', 'Node-Function-Object', 'class', 'hierarchy', 'are', 'proposed.', 'A', 'formalized', 'NFO/UFO', 'analysis', 'algorithm', 'and', 'CASE', 'tools', 'used', 'to', 'support', 'it', 'are', 'considered']\n",
      "==============keyword_tokens===========\n",
      "formal system representation;functional flow-type objects;formalized nfo/ufo analysis algorithm;systemological analysis;case tools;object-oriented design;axiomatics;complex systems design\n",
      "==============given_keyword_tokens===========\n",
      "[]\n",
      "==============pred_keyphrases===========\n",
      "['object', 'calculus of systems', 'axiomatics', 'vestn', 'new method of systemological analysis', 'formal system representation', 'results of development', 'case tools', 'khgpu', 'analysis']\n"
     ]
    }
   ],
   "source": [
    "# 整理tfidf的pred文件\n",
    "\n",
    "## 现在tfidf的pred文件格式是\n",
    "dict_list = read_jsonline('/mnt/KeyphraseExpansion/data/raw/inspec/inspec_pred_tfidf.json')\n",
    "for k,v in dict_list[1].items():\n",
    "    print('=============='+k+'===========')\n",
    "    print(v[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========title_and_abstract_tokens============\n",
      "['outlier', 'resistant', 'adaptive', 'matched', 'filtering', 'robust', 'adaptive', 'matched', 'filtering', '(', 'amf', ')', 'whereby', 'outlier', 'data', 'vectors', 'are', 'censored', 'from', 'the', 'covariance', 'matrix', 'estimate', 'is', 'considered', 'in', 'a', 'maximum', 'likelihood', 'estimation', '(', 'mle', ')', 'setting', '.', 'it', 'is', 'known', 'that', 'outlier', 'data', 'vectors', 'whose', 'steering', 'vector', 'is', 'highly', 'correlated', 'with', 'the', 'desired', 'steering', 'vector', ',', 'can', 'significantly', 'degrade', 'the', 'performance', 'of', 'amf', 'algorithms', 'such', 'as', 'sample', 'matrix', 'inversion', '(', 'smi', ')', 'or', 'fast', 'maximum', 'likelihood', '(', 'fml', ')', '.', 'four', 'new', 'algorithms', 'that', 'censor', 'outliers', 'are', 'presented', 'which', 'are', 'derived', 'via', 'approximation', 'to', 'the', 'mle', 'solution', '.', 'two', 'algorithms', 'each', 'are', 'related', 'to', 'using', 'the', 'smi', 'or', 'the', 'fml', 'to', 'estimate', 'the', 'unknown', 'underlying', 'covariance', 'matrix', '.', 'results', 'are', 'presented', 'using', 'computer', 'simulations', 'which', 'demonstrate', 'the', 'relative', 'effectiveness', 'of', 'the', 'four', 'algorithms', 'versus', 'each', 'other', 'and', 'also', 'versus', 'the', 'smi', 'and', 'fml', 'algorithms', 'in', 'the', 'presence', 'of', 'outliers', 'and', 'no', 'outliers', '.', 'it', 'is', 'shown', 'that', 'one', 'of', 'the', 'censoring', 'algorithms', ',', 'called', 'the', 'reiterative', 'censored', 'fast', 'maximum', 'likelihood', '(', 'cfml', ')', 'technique', 'is', 'significantly', 'superior', 'to', 'the', 'other', 'three', 'censoring', 'methods', 'in', 'stressful', 'outlier', 'scenarios']\n",
      "===========keyword_tokens============\n",
      "[['fast', 'maximum', 'likelihood'], ['steering', 'vector'], ['covariance', 'matrix', 'estimate'], ['reiterative', 'censored', 'fast', 'maximum', 'likelihood'], ['maximum', 'likelihood', 'estimation', 'setting']]\n",
      "===========given_keyword_tokens============\n",
      "[['censoring', 'algorithms'], ['outlier', 'resistant', 'adaptive', 'matched', 'filtering'], ['sample', 'matrix', 'inversion']]\n",
      "===========pred_keyphrases============\n",
      "[['adaptive', 'matched', 'filtering'], ['outlier', 'data', 'vectors'], ['maximum', 'likelihood', 'estimation'], ['censored', 'fast', 'maximum', 'likelihood', '(', 'fml', ')'], ['maximum', 'likelihood', 'estimation', '(', 'fml', ')'], ['adaptive', 'matched', 'filtering', '(', 'fml', ')'], ['fast', 'maximum', 'likelihood', 'estimation'], ['fast', 'maximum', 'likelihood'], ['outlier', 'resistant', 'fast', 'maximum', 'likelihood', '(', 'fml', ')'], ['adaptive', 'matched', 'filter'], ['adaptive', 'fast', 'maximum', 'likelihood', '(', 'fml', ')'], ['outlier'], ['covariance', 'matrix', 'estimate'], ['maximum', 'likelihood', 'estimation', '(', 'mle', ')'], ['robust', 'adaptive', 'matched', 'filtering', '(', 'fml', ')'], ['fast', 'maximum', 'likelihood', '(', 'fml', ')'], ['robust', 'adaptive', 'matched', 'filtering'], ['outlier', 'scenarios'], ['<unk>', 'censored', 'fast', 'maximum', 'likelihood', '(', 'fml', ')'], ['matched', 'filtering'], ['outlier', 'detection'], ['maximum', 'likelihood'], ['censoring'], ['censored', 'fast', 'maximum', 'likelihood'], ['robust', 'adaptive', 'matched', 'filter'], ['censoring', 'algorithms']]\n"
     ]
    }
   ],
   "source": [
    "## 我们需要的pred文件是\n",
    "dict_list = read_jsonline('/mnt/KeyphraseExpansion/data/jsonl/inspec/inspec_pred.jsonl')\n",
    "for k,v in dict_list[1].items():\n",
    "    print('==========='+k+'============')\n",
    "    print(v[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title_and_abstract_tokens可以不变\n",
    "# keyword_tokens，先按分号分开，再按空格分开\n",
    "# given：不变\n",
    "# pred：先按元素分开，每个元素是一个列表，再按空格分开\n",
    "\n",
    "def json_jsonl(json_file, jsonl_file):\n",
    "    dict_list = read_jsonline(json_file)\n",
    "    for dd in dict_list:\n",
    "        dd['title_and_abstract_tokens'] = dd['title_and_abstract_tokens']\n",
    "        keys = []\n",
    "        for keyphrase in dd['keyword_tokens'].split(';'):\n",
    "            keys.append(keyphrase.split())\n",
    "        dd['keyword_tokens'] = keys\n",
    "        dd['given_keyword_tokens'] = []\n",
    "        keys = []\n",
    "        for key in dd['pred_keyphrases']:\n",
    "            keys.append(key.split())\n",
    "        dd['pred_keyphrases'] = keys\n",
    "\n",
    "    for dic in dict_list:\n",
    "        filename = jsonl_file\n",
    "        append_line(filename, json.dumps(dic))\n",
    "\n",
    "datasets = ['inspec','krapivin','nus','semeval','kp20k']\n",
    "\n",
    "for data in datasets:\n",
    "    json_file = '/mnt/KeyphraseExpansion/data/raw/'+data+'/'+data+'_pred_textrank.json'\n",
    "    jsonl_file = '/mnt/KeyphraseExpansion/data/jsonl/'+data+'/'+data+'_pred_textrank.jsonl'\n",
    "    json_jsonl(json_file, jsonl_file)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0):\n",
    "    print('222')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.floor(0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
